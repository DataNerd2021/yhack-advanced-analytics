{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize/Tokenize scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Vectorize import create_vector_dict, vectorize_comment, vectorize_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords:\n",
      "{'', '!', 'j', '<', '46', 'move', '98', 'which', 'therefore', 'already', '17', '2', 'full', '\\\\{', 'than', '70', '1995', 'none', 'found', 'my', 'neither', 'system', 'hasnt', 'hers', 'noone', '35', '|', 'part', 'whither', 'yours', 'after', 'de', 'fill', 'thus', '32', 'whence', '76', '1993', 'made', 'see', 'above', '0', 'done', '37', 'moreover', 'though', '1998', '42', '28', '92', 'perhaps', 'twelve', '89', 'un', 'herself', 'sixty', 'c', \"doesn't\", 'herein', '77', 'along', 'beside', 'his', 'whoever', '43', 'please', 'her', 'its', '57', 'thru', '2017', 'indeed', 'five', 'somewhere', 'this', 'fifteen', 'l', '@', ';', 'that', 'hundred', 'y', 'all', 'did', '40', 'however', 'anywhere', 'hereafter', 'no', 'on', '2014', 'they', 'whenever', 'still', 'hence', 'latter', '45', 'whereupon', 'something', '>', 'others', 'nine', 'us', '2004', 'should', 'been', '$', '93', 'are', '2010', 'must', 'eleven', 'fify', '27', '4', 'himself', 'per', 'twenty', 'sometime', '52', 'there', 'm', '99', 'ourselves', 'k', '66', '53', 'around', 'ever', 'first', '\\\\+', 'myself', '2001', 'otherwise', 'q', '61', 'had', '2013', 'ltd', 'your', 'r', 'four', 'thereafter', 'eight', 'amongst', '2005', 'cant', 'afterwards', 'least', '73', '69', 'further', 'serious', '11', 'ours', '90', '60', 'p', 'when', 'into', 's', 'find', 'is', 't', 'but', 'forty', '54', 'alone', 'whether', 'everyone', 'it', 'do', 'he', 'only', '18', '\\\\]', 'does', 'namely', 'o', 'throughout', 'at', 'across', 'either', 'toward', 'always', 'other', 'six', 'hereby', 'both', '100', 'you', 'whereafter', 'put', '&', 'since', '63', '74', '2011', 'two', 'against', 'e', 'off', 'many', 'over', 'here', '\\\\*', '1990', 'hereupon', 'w', 'latterly', '31', 'whose', ':', 'f', '33', '1997', 'through', 'each', 'x', 'mostly', 'few', 'everywhere', '48', 'amoungst', '39', 'even', 'upon', '1996', 'rather', '1991', '26', 'up', 'me', '7', 'get', '91', '29', 'some', \"didn't\", 'former', 'how', 'wherever', 'd', 'same', 'these', '85', 'out', 'if', '16', 'else', '1992', 'couldnt', '19', 'whom', 'itself', '82', '13', 'fire', '1999', 'etc', 'due', 'during', 'sometimes', 'yourself', '3', '9', 'third', 'to', '22', '86', 'never', 'less', 'mill', 'will', '1', '97', 'whereby', 'give', 'name', '30', 'anyhow', '2018', 'nevertheless', '87', 'very', 'everything', 'by', 'cannot', 'in', 'may', 'about', 'onto', 'yourselves', 'down', '2016', 'ie', 'much', 'such', 'take', 'again', 'often', '94', 'formerly', 'h', 'nobody', '83', 'most', '72', 'who', '36', '2015', '\\\\?', 'him', 'keep', 'not', '\\\\[', '56', '21', 'thereupon', 'whatever', 'enough', 'whereas', 'nowhere', 'then', '10', 'beforehand', '8', 'anyway', 'why', 'before', 'one', 'except', 'nor', '.', '25', 'elsewhere', 'themselves', 'g', ',', '88', '96', '\\\\)', 'another', '55', 'the', 'until', 'because', 'well', '14', 'now', 'every', 'were', 'next', 'under', '2008', 'thereby', 'u', 'could', 'for', 'am', 'bottom', '1994', 'a', '-', 'also', '95', 'being', 'more', '2020', '12', '59', 'back', '49', 'anyone', 'with', '51', 'was', '2000', 'or', '68', '2012', '\\\\(', 'any', 'although', '2006', 'ten', 'con', 'inc', 'seem', '41', '78', 'those', 'therein', 'last', 'within', 'z', 'once', 'via', 'we', '#', '44', 'almost', 'so', '\\\\}', 'n', '34', 'while', 'would', 'mine', '\\\\^', 'too', 'thing', 'besides', 'someone', '2009', '65', '2019', 'and', 'somehow', 'several', 'co', 'seemed', 'whole', 'yet', '6', 'go', 'call', 'without', 'our', 'as', '50', 'where', '2007', '2002', 'together', 'among', 'between', 'front', 'can', 'three', 'she', \"don't\", '75', '71', '81', 'from', 'seems', 'anything', '15', 'has', '%', 'meanwhile', 'nothing', 're', 'what', 'might', 'an', 'i', '47', '67', 'wherein', '62', 'v', 'towards', '24', '5', 'thence', '58', '80', 'them', 'eg', '64', 'of', '23', 'own', 'be', '38', 'b', '2003', 'their', '84', 'seeming', '20', 'have', 'amount'}\n"
     ]
    }
   ],
   "source": [
    "def read_file_to_list(fpath: str) -> set[str]:\n",
    "    return set(open(fpath, 'r').read().split('\\n'))\n",
    "\n",
    "fpath = 'stopwords.txt'\n",
    "STOPWORDS = read_file_to_list(fpath)\n",
    "\n",
    "# Display stopwords\n",
    "print(\"Stopwords:\")\n",
    "print(STOPWORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('comments.csv').dropna()\n",
    "comments = df['Comment'].to_list()\n",
    "bagOfWords = create_vector_dict(comments, STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1:46:08': 0,\n",
       " 'https://supertutortv.com/sat/whats-a-good-sat-score-updated-2022-2023/': 1,\n",
       " 'repeat..that': 2,\n",
       " 'itens': 3,\n",
       " 'liars': 4,\n",
       " 'mystery': 5,\n",
       " '9:30': 6,\n",
       " \"monitor's\": 7,\n",
       " '(785.753': 8,\n",
       " 'DEFINITELY': 9,\n",
       " 'ficar': 10,\n",
       " 'vi,': 11,\n",
       " 'delightful': 12,\n",
       " 'Businessman,': 13,\n",
       " 'furry': 14,\n",
       " 'phenomenon,': 15,\n",
       " 'motivates': 16,\n",
       " 'uploading,': 17,\n",
       " 'rÃ¡pido': 18,\n",
       " '--': 19,\n",
       " 'Matter': 20,\n",
       " 'There.': 21,\n",
       " 'Kristoff': 22,\n",
       " 'lovely,': 23,\n",
       " 'mocking': 24,\n",
       " 'nice.': 25,\n",
       " 'out..smh': 26,\n",
       " 'preeminent': 27,\n",
       " 'But:': 28,\n",
       " 'arcâ€¦..': 29,\n",
       " 'strengths?': 30,\n",
       " 'reconnaissance': 31,\n",
       " 'IRL': 32,\n",
       " 'soon.': 33,\n",
       " 'Nokia': 34,\n",
       " 'lips!': 35,\n",
       " 'Practice': 36,\n",
       " '36.': 37,\n",
       " 'Relativity:': 38,\n",
       " 'overview.': 39,\n",
       " 'Hahah!': 40,\n",
       " 'Nano': 41,\n",
       " 'strands,': 42,\n",
       " 'Government:': 43,\n",
       " '$800,': 44,\n",
       " 'alleviating': 45,\n",
       " '(Transistors': 46,\n",
       " 'ðŸ¬': 47,\n",
       " 'Puedes': 48,\n",
       " 'majority': 49,\n",
       " 'Jusko': 50,\n",
       " 'dog/eat': 51,\n",
       " 'poðŸ˜ðŸ™ðŸ™ðŸ™': 52,\n",
       " 'underscore': 53,\n",
       " 'Integer:': 54,\n",
       " 'Bead': 55,\n",
       " 'Peterson': 56,\n",
       " '*literally*': 57,\n",
       " 'misses': 58,\n",
       " 'excepting': 59,\n",
       " 'arguments': 60,\n",
       " 'row': 61,\n",
       " 'ðŸ¤”': 62,\n",
       " 'absorbing': 63,\n",
       " 'Ver': 64,\n",
       " 'SeÃ±orita': 65,\n",
       " 'turf,': 66,\n",
       " 'oxymoron)': 67,\n",
       " 'artillery': 68,\n",
       " 'side-by-side': 69,\n",
       " 'GoFundMe:': 70,\n",
       " 'photographers': 71,\n",
       " 'film!': 72,\n",
       " 'Ã–vp': 73,\n",
       " 'w/leading': 74,\n",
       " 'uh..': 75,\n",
       " 'Courses:-': 76,\n",
       " 'stalls': 77,\n",
       " '1:04:56': 78,\n",
       " 'à¤­à¤¾à¤ˆ': 79,\n",
       " 'potential.': 80,\n",
       " \"'Signal\": 81,\n",
       " 'fan.': 82,\n",
       " 'list': 83,\n",
       " 'office,I': 84,\n",
       " 'adviser.': 85,\n",
       " 'author?': 86,\n",
       " 'slapping': 87,\n",
       " 'coments': 88,\n",
       " 'before)': 89,\n",
       " 'Viserys.': 90,\n",
       " 'nothing.': 91,\n",
       " 'Ø¹Ø§Ù„ÛŒ': 92,\n",
       " 'bed!!': 93,\n",
       " 'assert*': 94,\n",
       " 'IIMs,': 95,\n",
       " \"Ty's\": 96,\n",
       " 'shoter': 97,\n",
       " '(Roll': 98,\n",
       " 'ties': 99,\n",
       " \"again!'\": 100,\n",
       " 'mythical': 101,\n",
       " '39:47': 102,\n",
       " 'predated': 103,\n",
       " 'pragg..it': 104,\n",
       " 'wooded': 105,\n",
       " 'changes!': 106,\n",
       " 'Argentina': 107,\n",
       " 'experimenting': 108,\n",
       " 'Saudade': 109,\n",
       " 'story,': 110,\n",
       " 'musk:': 111,\n",
       " 'people._': 112,\n",
       " 'sikhs.proud': 113,\n",
       " 'disappoints': 114,\n",
       " 'studied,': 115,\n",
       " 'gerÃ§ekten': 116,\n",
       " '\\U0001faf6ðŸ»ðŸ˜‹': 117,\n",
       " 'aggravating': 118,\n",
       " 'Delhi': 119,\n",
       " '6:08': 120,\n",
       " 'choices!': 121,\n",
       " 'bonus': 122,\n",
       " 'repay': 123,\n",
       " 'blocked': 124,\n",
       " 'readers': 125,\n",
       " 'Em': 126,\n",
       " 'Philosophy': 127,\n",
       " 'media': 128,\n",
       " 'https://whatnot.com/invite/tronicsfix': 129,\n",
       " 'lmao)': 130,\n",
       " '60hz': 131,\n",
       " 'virtues': 132,\n",
       " 'novo': 133,\n",
       " 'ðŸ˜‚ðŸ‘ðŸ»ðŸ‘ŒðŸ»ðŸ‘ŒðŸ»': 134,\n",
       " 'chess': 135,\n",
       " 'SONIC': 136,\n",
       " 'à°µà±‡à°Ÿ': 137,\n",
       " 'value.': 138,\n",
       " 'felicito,': 139,\n",
       " '2M': 140,\n",
       " 'technologically,': 141,\n",
       " 'touching': 142,\n",
       " 'PHD': 143,\n",
       " 'Ð½Ð¾': 144,\n",
       " 'tutorials.': 145,\n",
       " 'cukup': 146,\n",
       " \"Michael's\": 147,\n",
       " 'streaming': 148,\n",
       " 'games,': 149,\n",
       " 'well),': 150,\n",
       " 'Ignore': 151,\n",
       " \"Exist'\": 152,\n",
       " '46:59': 153,\n",
       " 'extension?': 154,\n",
       " \"Y'all\": 155,\n",
       " 'poliðŸ˜ŽðŸ˜Ž': 156,\n",
       " 'bumper': 157,\n",
       " 'bongcloud': 158,\n",
       " 'arrancara': 159,\n",
       " 'photographs': 160,\n",
       " '(somewhere': 161,\n",
       " \"day'\": 162,\n",
       " 'romanticize': 163,\n",
       " 'deis': 164,\n",
       " 'Hamza.': 165,\n",
       " 'Impresionante!!!!': 166,\n",
       " 'preserved,': 167,\n",
       " 'CONTENTS': 168,\n",
       " 'Microsoft)': 169,\n",
       " 'period?': 170,\n",
       " 'see...': 171,\n",
       " 'UM-2': 172,\n",
       " 'acted': 173,\n",
       " '48:49': 174,\n",
       " 'hopefully': 175,\n",
       " 'babyðŸ˜ðŸ˜ðŸˆ\\u200dâ¬›ðŸˆðŸ¥°ðŸ¥°ðŸ¦®â™¥ï¸ðŸ•\\u200dðŸ¦ºðŸ’•ðŸ˜˜': 176,\n",
       " 'przygodÄ™': 177,\n",
       " 'parabÃ©ns': 178,\n",
       " 'effect': 179,\n",
       " 'streamlines': 180,\n",
       " 'trad!ng': 181,\n",
       " 'understood,': 182,\n",
       " '37:33': 183,\n",
       " 'Centres': 184,\n",
       " 'cannons.': 185,\n",
       " 'Frome': 186,\n",
       " 'VOCÃŠ': 187,\n",
       " '2.SQL': 188,\n",
       " 'pterosaur-like': 189,\n",
       " '($20': 190,\n",
       " 'goat': 191,\n",
       " 'HELLðŸ¤¬ðŸ˜¡Kids': 192,\n",
       " 'ÙˆÙŠØ¬Ø²Ø§Ùƒ': 193,\n",
       " 'acessos.': 194,\n",
       " 'fetch': 195,\n",
       " 'bugs,': 196,\n",
       " 'lewis': 197,\n",
       " 'screwing': 198,\n",
       " 'noneâ€': 199,\n",
       " 'Basically': 200,\n",
       " '3,4': 201,\n",
       " 'theirs!': 202,\n",
       " 'atriz': 203,\n",
       " 'videolarÄ±': 204,\n",
       " 'inflict': 205,\n",
       " 'Nutty': 206,\n",
       " 'knowledge': 207,\n",
       " 'í›ˆë‚¨ì—': 208,\n",
       " 'project.': 209,\n",
       " 'â¤â¤â¤ðŸ‘ðŸ‘ðŸ‘ðŸ‡¦ðŸ‡ªðŸ‡±ðŸ‡°ðŸ‡¦ðŸ‡ªðŸ‡±ðŸ‡°': 210,\n",
       " 'gates': 211,\n",
       " 'kyo,': 212,\n",
       " 'al': 213,\n",
       " 'Seetoh.': 214,\n",
       " 'curse': 215,\n",
       " 'pc,': 216,\n",
       " 'Ø¶Ø­Ùƒ': 217,\n",
       " 'that,.': 218,\n",
       " 'Client': 219,\n",
       " 'https://www.tiktok.com/@mrchashundley/video/7100196164024274218': 220,\n",
       " 'lit!': 221,\n",
       " 'https://bit.ly/Binance-MZG': 222,\n",
       " 'SUCH': 223,\n",
       " 'kgb': 224,\n",
       " 'upðŸ˜†Indict': 225,\n",
       " 'partes': 226,\n",
       " 'indifference': 227,\n",
       " 'CSS': 228,\n",
       " 'qxmg.': 229,\n",
       " 'yeniden': 230,\n",
       " 'IRELAND': 231,\n",
       " 'â€œmore': 232,\n",
       " 'blades': 233,\n",
       " 'pouch': 234,\n",
       " 'Chase': 235,\n",
       " '4x': 236,\n",
       " '(probably': 237,\n",
       " 'czarnÄ…': 238,\n",
       " 'outstanding!': 239,\n",
       " 'UG': 240,\n",
       " 'EAM/ELAS': 241,\n",
       " 'CONTROL': 242,\n",
       " '-so,': 243,\n",
       " 'jazz': 244,\n",
       " 'CAME': 245,\n",
       " 'LTT.': 246,\n",
       " 'lecturer': 247,\n",
       " 'ìž¬ë°Œê²Œ': 248,\n",
       " 'Kyla,': 249,\n",
       " 'indicted': 250,\n",
       " 'mathematical': 251,\n",
       " 'magistrate': 252,\n",
       " 'shocked..': 253,\n",
       " 'game,': 254,\n",
       " 'Siri': 255,\n",
       " '4:06': 256,\n",
       " 'eso': 257,\n",
       " '8:18': 258,\n",
       " 'reveal...': 259,\n",
       " 'bounds': 260,\n",
       " 'authentic,': 261,\n",
       " 'kanoon,': 262,\n",
       " 'communities': 263,\n",
       " 'completely,': 264,\n",
       " 'sir!!!': 265,\n",
       " 'NP.': 266,\n",
       " '3.1k,': 267,\n",
       " 'study,': 268,\n",
       " 'valuable..thank': 269,\n",
       " 'permanent': 270,\n",
       " 'Wednesday.': 271,\n",
       " 'compilationâ€': 272,\n",
       " 'competence': 273,\n",
       " 'careers.': 274,\n",
       " 'desktop,': 275,\n",
       " 'Ikora': 276,\n",
       " 'messy': 277,\n",
       " 'cheeseburger,': 278,\n",
       " 'DD': 279,\n",
       " 'Gremlins': 280,\n",
       " 'attach': 281,\n",
       " 'venha': 282,\n",
       " 'headâ€': 283,\n",
       " 'Olympics.': 284,\n",
       " '06:00': 285,\n",
       " 'nintendo.': 286,\n",
       " 'vocalise': 287,\n",
       " '3.8': 288,\n",
       " 'officially': 289,\n",
       " '1:34:58': 290,\n",
       " 'estemos': 291,\n",
       " '(24)': 292,\n",
       " 'slows,': 293,\n",
       " 'Siamese': 294,\n",
       " 'worlwide': 295,\n",
       " 'preston': 296,\n",
       " 'position': 297,\n",
       " 'uses': 298,\n",
       " 'http://3commas.kdub.co': 299,\n",
       " '10m': 300,\n",
       " 'ðŸ‘ðŸ’•': 301,\n",
       " 'DUDE!': 302,\n",
       " 'ðŸ¥°âœ¨ðŸ™Œâœ¨': 303,\n",
       " '0:09': 304,\n",
       " 'Controlled': 305,\n",
       " 'inequalities': 306,\n",
       " '1:20:38': 307,\n",
       " 'need.': 308,\n",
       " 'therapy.': 309,\n",
       " 'room,': 310,\n",
       " 'riding;': 311,\n",
       " 'bar,': 312,\n",
       " 'gadgets,': 313,\n",
       " 'Max-Heaps': 314,\n",
       " 'ë¯¸ì³¤ë‹¤..í•­ìƒ': 315,\n",
       " 'Statements': 316,\n",
       " 'figure,my': 317,\n",
       " 'Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸ÐµðŸ‘ðŸ‘ðŸ‘': 318,\n",
       " 'upsetting.': 319,\n",
       " 'queria': 320,\n",
       " 'â€¼ï¸â€¼ï¸â€¼ï¸': 321,\n",
       " 'deviation.': 322,\n",
       " 'punching': 323,\n",
       " 'even!': 324,\n",
       " 'bueno': 325,\n",
       " 'awakened': 326,\n",
       " 'Finland,': 327,\n",
       " 'hornets': 328,\n",
       " 'Preis': 329,\n",
       " 'guts,': 330,\n",
       " 'shot': 331,\n",
       " 'imprint': 332,\n",
       " 'borrowed': 333,\n",
       " 'Continue': 334,\n",
       " 'bought.': 335,\n",
       " 'Governments': 336,\n",
       " 'Carlos': 337,\n",
       " 'Dhan': 338,\n",
       " '(Keep': 339,\n",
       " 'Casha': 340,\n",
       " 'annoyed': 341,\n",
       " 'toÃ n': 342,\n",
       " 'prettiest': 343,\n",
       " 'gosh': 344,\n",
       " 'Thunderclap': 345,\n",
       " 'Behavior': 346,\n",
       " 'Reddy': 347,\n",
       " 'Ä‘á»‹nh': 348,\n",
       " '(NASNet)': 349,\n",
       " 'cÃ´mico,': 350,\n",
       " 'Agriculture': 351,\n",
       " 'parting': 352,\n",
       " 'hexagons,': 353,\n",
       " 'Unfortunately': 354,\n",
       " '2:47:37': 355,\n",
       " 'reused': 356,\n",
       " 'vicious': 357,\n",
       " 'part!': 358,\n",
       " 'subreddits': 359,\n",
       " 'af.': 360,\n",
       " 'ê¶ê¸ˆí•˜ì‹œë‹¤êµ¬ìš”?': 361,\n",
       " 'sai):': 362,\n",
       " '$14.99/year.': 363,\n",
       " 'nerd': 364,\n",
       " 'plants?': 365,\n",
       " '[Monstercat': 366,\n",
       " 'TOUR': 367,\n",
       " 'imo,': 368,\n",
       " 'Think...!': 369,\n",
       " 'patriots,': 370,\n",
       " 'gatos': 371,\n",
       " 'example!': 372,\n",
       " 'her:': 373,\n",
       " '2:42-AirPods': 374,\n",
       " 'Arceus': 375,\n",
       " 'Fewer': 376,\n",
       " 'freedomðŸ¤”': 377,\n",
       " 'Generation.\"': 378,\n",
       " 'Avatar,': 379,\n",
       " 'gamestop': 380,\n",
       " 'KIDDING?.': 381,\n",
       " 'KNOWS': 382,\n",
       " 'MANÃˆ': 383,\n",
       " 'therapist': 384,\n",
       " 'knowledge.\"': 385,\n",
       " 'Rise:': 386,\n",
       " 'whatching': 387,\n",
       " 'charging.': 388,\n",
       " 'steady,': 389,\n",
       " 'Songbirds': 390,\n",
       " 'Ù…Ø§': 391,\n",
       " 'really..but': 392,\n",
       " 'however,': 393,\n",
       " 'khÃ¡c': 394,\n",
       " 'many..,': 395,\n",
       " 'infos': 396,\n",
       " '28:25': 397,\n",
       " '(It': 398,\n",
       " 'breakfast).': 399,\n",
       " 'Trendy': 400,\n",
       " 'stability.': 401,\n",
       " 'pinks': 402,\n",
       " 'anjum': 403,\n",
       " 'lay': 404,\n",
       " 'Betaal': 405,\n",
       " 'MARKETING': 406,\n",
       " 'Old': 407,\n",
       " 'Convention.': 408,\n",
       " 'Leader': 409,\n",
       " 'episode.': 410,\n",
       " 'Ñ‚Ð°Ð¼,': 411,\n",
       " 'calculate': 412,\n",
       " 'shipped': 413,\n",
       " 'coming!': 414,\n",
       " 'sentence.': 415,\n",
       " 'us......': 416,\n",
       " 'Persons\"': 417,\n",
       " 'Axis': 418,\n",
       " 'opened': 419,\n",
       " 'bragging': 420,\n",
       " 'facility': 421,\n",
       " 'egg?': 422,\n",
       " 'vivendo': 423,\n",
       " 'agent': 424,\n",
       " '2:43': 425,\n",
       " 'recently-': 426,\n",
       " 'health': 427,\n",
       " 'veggies': 428,\n",
       " 'wish.': 429,\n",
       " 'goal': 430,\n",
       " 'highlights,': 431,\n",
       " 'BenoftheweekðŸ˜©': 432,\n",
       " 'suele': 433,\n",
       " 'enjoy,': 434,\n",
       " 'Piano': 435,\n",
       " 'infant': 436,\n",
       " '!ðŸ‡¹ðŸ‡¼': 437,\n",
       " 'instructive!': 438,\n",
       " 'vehemently': 439,\n",
       " 'makakuha': 440,\n",
       " 'times.': 441,\n",
       " 'Crazy.': 442,\n",
       " '\"Out': 443,\n",
       " 'Django': 444,\n",
       " 'ÐšÐ°Ðº': 445,\n",
       " 'estate.': 446,\n",
       " 'ðŸ˜ðŸ‡§ðŸ‡·ðŸŽ¶': 447,\n",
       " '13:31': 448,\n",
       " 'deta': 449,\n",
       " 'Faz': 450,\n",
       " 'Percentiles': 451,\n",
       " 'Jerry,': 452,\n",
       " 'TwT': 453,\n",
       " '4:53:10': 454,\n",
       " 'gas).': 455,\n",
       " 'ever..ðŸ™Œ': 456,\n",
       " 'occupation,': 457,\n",
       " 'take:': 458,\n",
       " '(over': 459,\n",
       " 'direction!': 460,\n",
       " 'marenda': 461,\n",
       " 'stateflow': 462,\n",
       " 'pivoted:': 463,\n",
       " 'usðŸ¥ºðŸ¤—': 464,\n",
       " 'adressed,': 465,\n",
       " 'takers.': 466,\n",
       " 'indefensible': 467,\n",
       " 'Octavius': 468,\n",
       " 'reservation,': 469,\n",
       " 'Philosopher...........': 470,\n",
       " 'golem': 471,\n",
       " 'pocket,': 472,\n",
       " 'offended': 473,\n",
       " '\"aah': 474,\n",
       " '8:35': 475,\n",
       " 'wills.\"': 476,\n",
       " 'striving': 477,\n",
       " 'Grayscale': 478,\n",
       " 'Godot': 479,\n",
       " 'OTRA': 480,\n",
       " 'Douglas': 481,\n",
       " 'homage': 482,\n",
       " 'Piper,': 483,\n",
       " 'Tide)': 484,\n",
       " 'childhood': 485,\n",
       " 'Trash': 486,\n",
       " 'ambitious,': 487,\n",
       " \"'false\": 488,\n",
       " 'Jesse': 489,\n",
       " '#1': 490,\n",
       " 'Beat': 491,\n",
       " 'lipstick': 492,\n",
       " 'gracefully.': 493,\n",
       " 'eel': 494,\n",
       " 'Casi': 495,\n",
       " 'Tibet.': 496,\n",
       " 'discusses': 497,\n",
       " 'â€œya': 498,\n",
       " 'Mrbeast,': 499,\n",
       " 'check': 500,\n",
       " 'col': 501,\n",
       " 'State,': 502,\n",
       " 'understands,': 503,\n",
       " 'improvement,': 504,\n",
       " 'incentives': 505,\n",
       " 'acualy': 506,\n",
       " 'regulation.': 507,\n",
       " 'Cooker': 508,\n",
       " 'gimmick...': 509,\n",
       " 'structures,': 510,\n",
       " 'posts.': 511,\n",
       " 'Aemma.': 512,\n",
       " 'Leroy': 513,\n",
       " 'Bohm.': 514,\n",
       " 'nickelback,': 515,\n",
       " 'Odysseus,': 516,\n",
       " 'EQRIC': 517,\n",
       " 'definitions.': 518,\n",
       " 'applications,': 519,\n",
       " 'Abused': 520,\n",
       " 'captioning': 521,\n",
       " 'moneyâ€': 522,\n",
       " 'ç·´ç¿’ã®é‚ªé­”ã«ãªã‚‹ã‚‚ã®ã‚’å–ã‚Šé™¤ã': 523,\n",
       " 'conversation!!': 524,\n",
       " 'ow': 525,\n",
       " 'â€œthe': 526,\n",
       " 'HIRING,': 527,\n",
       " 'Hell.': 528,\n",
       " 'Lisa,': 529,\n",
       " 'bussiness': 530,\n",
       " '6:44': 531,\n",
       " 'Murdoch,': 532,\n",
       " 'báº¥m': 533,\n",
       " 'iPhones': 534,\n",
       " 'Structures?': 535,\n",
       " 'Jungkook': 536,\n",
       " 'OWNERâ€™S': 537,\n",
       " '*adds': 538,\n",
       " 'icreate': 539,\n",
       " 'okay,': 540,\n",
       " 'yet.': 541,\n",
       " 'scenario': 542,\n",
       " 'theme': 543,\n",
       " 'UtwÃ³r': 544,\n",
       " 'Adventure': 545,\n",
       " 'ÑƒÐ´Ð¾Ð²Ð¾Ð»ÑŒÑÑ‚Ð²Ð¸ÐµÐ¼': 546,\n",
       " 'DFRC': 547,\n",
       " '#Avreactshow': 548,\n",
       " 'anomalous': 549,\n",
       " '11:07': 550,\n",
       " 'ðŸ’”ðŸ’': 551,\n",
       " 'â€œHey!': 552,\n",
       " 'lag': 553,\n",
       " 'bell': 554,\n",
       " 'each.': 555,\n",
       " 'Thanks': 556,\n",
       " '1:06': 557,\n",
       " 'Wrong': 558,\n",
       " 'https://youtube.com/playlist?list=PLa4zmfXkL4P1egcgfYFHoXRb2Da_pKa2u': 559,\n",
       " 'finance,': 560,\n",
       " 'multimodal': 561,\n",
       " 'limiting': 562,\n",
       " '\"Still': 563,\n",
       " 'candidacy': 564,\n",
       " 'Problems': 565,\n",
       " 'spectacular': 566,\n",
       " '51.': 567,\n",
       " '\"dangerous\"': 568,\n",
       " 'piss': 569,\n",
       " 'steep': 570,\n",
       " '()': 571,\n",
       " 'prompting': 572,\n",
       " 'dangerous': 573,\n",
       " 'presentable': 574,\n",
       " 'CARTOON': 575,\n",
       " 'Yeet': 576,\n",
       " 'only?': 577,\n",
       " 'chÃ­nh': 578,\n",
       " 'Phoenix': 579,\n",
       " 'Key': 580,\n",
       " 'inflation': 581,\n",
       " 'pursuid': 582,\n",
       " 'thick,': 583,\n",
       " 'Zelda:': 584,\n",
       " 'notches.': 585,\n",
       " 'Guadalupian': 586,\n",
       " 'families.': 587,\n",
       " 'Drink:': 588,\n",
       " 'split': 589,\n",
       " 'completionist': 590,\n",
       " 'knew!!': 591,\n",
       " 'state),': 592,\n",
       " 'í¥ì‚¼ë‹˜': 593,\n",
       " 'Fahrenheit': 594,\n",
       " 'MSc': 595,\n",
       " 'higher)': 596,\n",
       " 'circumstances': 597,\n",
       " 'Teclado': 598,\n",
       " 'Quiz': 599,\n",
       " '1:27:13': 600,\n",
       " 'doctor.*': 601,\n",
       " 'ðŸ˜­ðŸ’•ðŸ’•ðŸ’•': 602,\n",
       " 'Tv,': 603,\n",
       " 'Evil': 604,\n",
       " 'INDONESIA': 605,\n",
       " 'size/weight,': 606,\n",
       " 'coisin': 607,\n",
       " '6:17-': 608,\n",
       " 'crisp,': 609,\n",
       " 'iddaru': 610,\n",
       " 'Dump': 611,\n",
       " 'Alot': 612,\n",
       " 'purple....': 613,\n",
       " 'enjoy!': 614,\n",
       " 'Sary': 615,\n",
       " 'fit': 616,\n",
       " 'VIDS': 617,\n",
       " 'confused,': 618,\n",
       " 'statement:': 619,\n",
       " 'VA)': 620,\n",
       " 'photon-photon': 621,\n",
       " 'solves': 622,\n",
       " 'movieðŸ˜¯ðŸ˜”': 623,\n",
       " 'semelhante,': 624,\n",
       " 'Weichs': 625,\n",
       " 'pinned': 626,\n",
       " 'monarchists': 627,\n",
       " 'yaÅŸadÄ±ÄŸÄ±n': 628,\n",
       " 'Americas;': 629,\n",
       " 'Atozy': 630,\n",
       " 'See': 631,\n",
       " '!!))': 632,\n",
       " '4:54:58...': 633,\n",
       " 'editions': 634,\n",
       " 'right,': 635,\n",
       " 'ignorant,': 636,\n",
       " \"Deadpool's\": 637,\n",
       " 'Lopez-Alt,': 638,\n",
       " 'slowest.': 639,\n",
       " 'gag': 640,\n",
       " 'killer*': 641,\n",
       " 'Quad': 642,\n",
       " 'harryâ€™s': 643,\n",
       " 'Developer': 644,\n",
       " '*Odysseus': 645,\n",
       " 'broken.': 646,\n",
       " 'espremetei': 647,\n",
       " 'botava': 648,\n",
       " 'desempeÃ±en': 649,\n",
       " 'graphen': 650,\n",
       " 'Wonderworld': 651,\n",
       " 'mam': 652,\n",
       " 'terminal': 653,\n",
       " 'voice)': 654,\n",
       " 'MLT': 655,\n",
       " 'AMENDMENT?â€': 656,\n",
       " 'Internship': 657,\n",
       " '$5,000': 658,\n",
       " 'incident': 659,\n",
       " 'showers.': 660,\n",
       " 'Lmao': 661,\n",
       " 'natie!': 662,\n",
       " 'Kellyâ€™s': 663,\n",
       " 'Ah': 664,\n",
       " 'mars': 665,\n",
       " '1350,': 666,\n",
       " '\"Entrepreneurs': 667,\n",
       " 'drunken': 668,\n",
       " 'Switch,': 669,\n",
       " 'E.g.,': 670,\n",
       " 'shree': 671,\n",
       " 'sparkly': 672,\n",
       " 'appearing.': 673,\n",
       " 'RMSE': 674,\n",
       " 'perfectionism': 675,\n",
       " 'housed': 676,\n",
       " \"th'\": 677,\n",
       " 'lindos': 678,\n",
       " '\"Do': 679,\n",
       " 'increasing': 680,\n",
       " 'repairservice': 681,\n",
       " 'BTC/ETH/BNB/ADA/MATIC...': 682,\n",
       " 'UP.': 683,\n",
       " 'mejoras': 684,\n",
       " 'elbows.': 685,\n",
       " 'investigating': 686,\n",
       " 'viewingsðŸ˜€.': 687,\n",
       " 'absolutley': 688,\n",
       " 'bigger': 689,\n",
       " 'loosen': 690,\n",
       " 'child,a': 691,\n",
       " 'ADHD.': 692,\n",
       " 'trÆ°á»ng': 693,\n",
       " 'nach': 694,\n",
       " \"Deck's\": 695,\n",
       " 'AND': 696,\n",
       " 'sloÅ¾it': 697,\n",
       " 'directors': 698,\n",
       " 'Choose': 699,\n",
       " 'Thumbnail!': 700,\n",
       " 'exist': 701,\n",
       " 'Buldozer': 702,\n",
       " 'SATA': 703,\n",
       " 'Constantinople': 704,\n",
       " 'opposing': 705,\n",
       " 'OMGGGG': 706,\n",
       " 'common': 707,\n",
       " 'xi,': 708,\n",
       " 'thunder.': 709,\n",
       " 'include': 710,\n",
       " 'clarification.': 711,\n",
       " 'range,ðŸ¥µ': 712,\n",
       " 'Descriptive': 713,\n",
       " 'allÃ­': 714,\n",
       " '53:07': 715,\n",
       " 'bewtween': 716,\n",
       " 'FL05': 717,\n",
       " 'quibble': 718,\n",
       " 'low-code': 719,\n",
       " 'ðŸŽ¼ðŸŽ¶ðŸŽ§ðŸ˜ŒðŸ‘': 720,\n",
       " 'scene.': 721,\n",
       " 'à¦§à¦°à§à¦®': 722,\n",
       " 'ðŸ˜ðŸ”¥': 723,\n",
       " 'Surely': 724,\n",
       " 'Pepper': 725,\n",
       " 'chef.': 726,\n",
       " 'ðŸ‡±ðŸ‡°': 727,\n",
       " '3~4': 728,\n",
       " 'Nepotism': 729,\n",
       " 'neighbors,': 730,\n",
       " 'CUZ': 731,\n",
       " 'daali': 732,\n",
       " '\"Sen': 733,\n",
       " 'Piece': 734,\n",
       " 'rediculous.': 735,\n",
       " '(due': 736,\n",
       " 'wasted': 737,\n",
       " 'Tax': 738,\n",
       " 'Kingdoms': 739,\n",
       " 'follow,': 740,\n",
       " 'desarrolladas.': 741,\n",
       " 'deter': 742,\n",
       " '\"accurate': 743,\n",
       " 'opens': 744,\n",
       " 'reliefðŸ’ž': 745,\n",
       " 'ÑÐ¿Ð°ÑÐ¸Ð±Ð¾,': 746,\n",
       " 'audios': 747,\n",
       " 'no-one': 748,\n",
       " 'smallest.': 749,\n",
       " 'cuboids,': 750,\n",
       " 'Roles': 751,\n",
       " 'Epa.': 752,\n",
       " 'mate,its': 753,\n",
       " 'Ivie': 754,\n",
       " 'Model,': 755,\n",
       " 'profit.': 756,\n",
       " 'A.': 757,\n",
       " 'Epico,': 758,\n",
       " 'quedaron': 759,\n",
       " 'clamour': 760,\n",
       " 'Perpetual': 761,\n",
       " 'service!': 762,\n",
       " '..K.i.s.s.': 763,\n",
       " 'Hitmaka': 764,\n",
       " 'VideoðŸ˜‚ðŸ˜‚ðŸ˜‚?': 765,\n",
       " 'engaging,': 766,\n",
       " 'Forces': 767,\n",
       " 'delivery.': 768,\n",
       " 'trucking': 769,\n",
       " 'mold': 770,\n",
       " 'SEXNUDE.UNO': 771,\n",
       " 'Xenomania': 772,\n",
       " 'event.': 773,\n",
       " 'everyday\"': 774,\n",
       " 'bishop.': 775,\n",
       " 'Praggananda': 776,\n",
       " 'cucharÃ³n': 777,\n",
       " 'à´…à´¦àµà´§àµà´µà´¾à´¨à´µàµà´‚à´•àµŠà´£àµà´Ÿàµ': 778,\n",
       " 'Vaseline': 779,\n",
       " 'cex': 780,\n",
       " 'London': 781,\n",
       " 'sandwiches': 782,\n",
       " 'Science-Fiction': 783,\n",
       " 'temps': 784,\n",
       " 'NAD+': 785,\n",
       " 'commensurate': 786,\n",
       " 'chillen': 787,\n",
       " 'Divisions.': 788,\n",
       " 'initial': 789,\n",
       " 'coordinates': 790,\n",
       " 'obliterate': 791,\n",
       " 'soak': 792,\n",
       " 'MAU,': 793,\n",
       " 'households': 794,\n",
       " 'aprende': 795,\n",
       " 'logo!!!': 796,\n",
       " 'vapes,': 797,\n",
       " 'ownersâ€™': 798,\n",
       " 'filtro': 799,\n",
       " 'curry': 800,\n",
       " 'wife,': 801,\n",
       " 'ê²ƒ': 802,\n",
       " 'trained.': 803,\n",
       " 'Morbin': 804,\n",
       " 'JJ': 805,\n",
       " 'Dominos': 806,\n",
       " '7:50-iPhone': 807,\n",
       " 'sizes.': 808,\n",
       " 'notebooks?': 809,\n",
       " '(looks': 810,\n",
       " 'Fats': 811,\n",
       " '\"THE': 812,\n",
       " 'http://pl.go-ga.me/59pk97zw': 813,\n",
       " 'explainedâ¤ï¸': 814,\n",
       " 'lifting': 815,\n",
       " 'miserable': 816,\n",
       " 'Rulfo': 817,\n",
       " 'crinkle': 818,\n",
       " 'ATHâ€™s.': 819,\n",
       " 'alone!': 820,\n",
       " 'exam': 821,\n",
       " 'Bol': 822,\n",
       " 'diesel': 823,\n",
       " 'respect...': 824,\n",
       " '..your': 825,\n",
       " 'logs': 826,\n",
       " 'recuerdos': 827,\n",
       " 'interviewer.': 828,\n",
       " 'Olymp': 829,\n",
       " 'â€œbut': 830,\n",
       " 'á‹á‹šá‰ƒ': 831,\n",
       " 'Cs': 832,\n",
       " 'unhatched': 833,\n",
       " '1:40:01': 834,\n",
       " 'aircrafts': 835,\n",
       " 'virginâ€': 836,\n",
       " 'saludo': 837,\n",
       " 'sentiment': 838,\n",
       " 'flowering': 839,\n",
       " 'australian': 840,\n",
       " 'MAN.': 841,\n",
       " 'wheb': 842,\n",
       " '00:37:05': 843,\n",
       " 'VII': 844,\n",
       " 'anyway,': 845,\n",
       " 'Coincedence?': 846,\n",
       " '(1:12:27)': 847,\n",
       " 'men.': 848,\n",
       " 'deeplearning.ai': 849,\n",
       " 'Christ,': 850,\n",
       " 'Prandini': 851,\n",
       " 'machinists': 852,\n",
       " 'ðŸ“±': 853,\n",
       " 'ì˜ìƒë“¤ë„': 854,\n",
       " 'next.': 855,\n",
       " 'NOTEBOOK': 856,\n",
       " '13:07': 857,\n",
       " 'Ela': 858,\n",
       " 'atores': 859,\n",
       " 'manually': 860,\n",
       " 'one-way': 861,\n",
       " '(1:00:25)': 862,\n",
       " 'slightly': 863,\n",
       " 'knows,': 864,\n",
       " '[Verse': 865,\n",
       " ':P': 866,\n",
       " 'stop': 867,\n",
       " 'MrBeast.': 868,\n",
       " 'ke': 869,\n",
       " 'avengers': 870,\n",
       " 'ë¬¸ì–´ì—': 871,\n",
       " \"Amitabh's\": 872,\n",
       " \"'Augustus'\": 873,\n",
       " 'Salmon': 874,\n",
       " ';-)': 875,\n",
       " 'reactions,': 876,\n",
       " 'sÃºper': 877,\n",
       " 'great)': 878,\n",
       " 'lulu-lemon': 879,\n",
       " 'amo!': 880,\n",
       " 'D80': 881,\n",
       " 'coal': 882,\n",
       " 'andereq': 883,\n",
       " 'army': 884,\n",
       " 'lawyers': 885,\n",
       " 'momentum,': 886,\n",
       " 'discussing': 887,\n",
       " 'gifts,': 888,\n",
       " 'Merci...': 889,\n",
       " 'hearts?': 890,\n",
       " 'queden': 891,\n",
       " 'scarlet': 892,\n",
       " 'reinforcements.': 893,\n",
       " 'Deal.': 894,\n",
       " 'OVERVIEW': 895,\n",
       " 'ðŸ˜ðŸ˜­ðŸ˜¡ðŸ˜¤': 896,\n",
       " 'Ä‘uá»•i': 897,\n",
       " 'calyxOS,': 898,\n",
       " 'fazer': 899,\n",
       " 'mesmo..': 900,\n",
       " 'opening!': 901,\n",
       " 'Hepburn,': 902,\n",
       " 'podium': 903,\n",
       " 'ABOUT': 904,\n",
       " 'trÃ¡iler!!!': 905,\n",
       " 'WAYYY': 906,\n",
       " 'house,': 907,\n",
       " 'call)': 908,\n",
       " 'nÃ£y': 909,\n",
       " 'Laundry,': 910,\n",
       " 'tristeza.': 911,\n",
       " 'entertainment,': 912,\n",
       " 'à¦¸à¦®à§Ÿà§‡à¦°': 913,\n",
       " 'insight': 914,\n",
       " 'ðŸ‘Like': 915,\n",
       " 'suggests.': 916,\n",
       " 'overlook': 917,\n",
       " 'managed': 918,\n",
       " 'Pille': 919,\n",
       " '7-8': 920,\n",
       " '(2016)': 921,\n",
       " 'jelly,': 922,\n",
       " 'à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¸µà¹‰à¸•à¹‰à¸­à¸‡à¹€à¸žà¸´à¹ˆà¸¡à¹‚à¸„à¸§à¸´à¸”-19à¸­à¸µà¸2-3à¸›à¸µ': 923,\n",
       " 'sempre.': 924,\n",
       " 'opinions.': 925,\n",
       " 'ps5': 926,\n",
       " '1:33:18': 927,\n",
       " '1:12:58': 928,\n",
       " 'gamers,': 929,\n",
       " 'Ana.': 930,\n",
       " 'Adorable': 931,\n",
       " 'cÃ¢meras': 932,\n",
       " '..I': 933,\n",
       " 'horribly,': 934,\n",
       " '06:12:44': 935,\n",
       " 'euphoria': 936,\n",
       " 'sinabing': 937,\n",
       " 'marvellously': 938,\n",
       " 'Pianote.': 939,\n",
       " 'talentsâ€': 940,\n",
       " 'ngk': 941,\n",
       " 'quicker.': 942,\n",
       " 'VEE-ta):': 943,\n",
       " '(average).': 944,\n",
       " 'oh....he': 945,\n",
       " 'peasy.\"': 946,\n",
       " 'pre-doctoral': 947,\n",
       " 'Besides,': 948,\n",
       " 'Vishal-Shekhar': 949,\n",
       " 'baffling': 950,\n",
       " 'next...': 951,\n",
       " 'europa,': 952,\n",
       " 'prayers,': 953,\n",
       " 'ceased': 954,\n",
       " 'Yum': 955,\n",
       " '5C,': 956,\n",
       " \"Cyclops'\": 957,\n",
       " 'â€œCheckerâ€': 958,\n",
       " '3:08:57': 959,\n",
       " \"'For\": 960,\n",
       " 'advent': 961,\n",
       " 'VOCÃŠS': 962,\n",
       " \"'di\": 963,\n",
       " 'programmers.': 964,\n",
       " '32-track': 965,\n",
       " 'crackling': 966,\n",
       " 'guide.': 967,\n",
       " 'prhase': 968,\n",
       " 'youâ€': 969,\n",
       " 'must,': 970,\n",
       " 'behind': 971,\n",
       " 'million.': 972,\n",
       " 'teeth-brushing': 973,\n",
       " 'levy': 974,\n",
       " 'paralysis': 975,\n",
       " 'aucune': 976,\n",
       " 'internetðŸŽ®ðŸ¦–â™£ï¸': 977,\n",
       " 'https://youtu.be/8DvywoWv6fI': 978,\n",
       " 'Percents': 979,\n",
       " 'else*': 980,\n",
       " 'Pilipinas!': 981,\n",
       " 'Lectures': 982,\n",
       " 'rather)': 983,\n",
       " 'ê²ƒë„': 984,\n",
       " 'hell.': 985,\n",
       " 'dáº¡o': 986,\n",
       " 'pregnantðŸ˜…': 987,\n",
       " 'Mysaria': 988,\n",
       " 'lincon': 989,\n",
       " 'Hoogway': 990,\n",
       " 'ðŸ»ðŸ•¹': 991,\n",
       " 'sein': 992,\n",
       " 'pierce': 993,\n",
       " 'starving.': 994,\n",
       " 'Pro?': 995,\n",
       " '52:00,': 996,\n",
       " 'Ãºnico,': 997,\n",
       " 'Ä‘Ã³': 998,\n",
       " 'elitist,': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#temp\n",
    "bagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I love NLTK! It's amazing.\n",
      "Sentiment Score: {'neg': 0.0, 'neu': 0.194, 'pos': 0.806, 'compound': 0.8516}\n",
      "Sentiment: Positive\n",
      "\n",
      "Sentence: This is a neutral statement.\n",
      "Sentiment Score: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Sentiment: Neutral\n",
      "\n",
      "Sentence: I dislike negative reviews.\n",
      "Sentiment Score: {'neg': 0.863, 'neu': 0.137, 'pos': 0.0, 'compound': -0.743}\n",
      "Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create a SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"I love NLTK! It's amazing.\",\n",
    "    \"This is a neutral statement.\",\n",
    "    \"I dislike negative reviews.\",\n",
    "]\n",
    "\n",
    "# Analyze sentiment for each sentence\n",
    "for sentence in sentences:\n",
    "    sentiment_score = sia.polarity_scores(sentence)\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Sentiment Score: {sentiment_score}\")\n",
    "    print(\"Sentiment:\", end=\" \")\n",
    "\n",
    "    # Determine sentiment based on compound score\n",
    "    if sentiment_score['compound'] >= 0.05:\n",
    "        print(\"Positive\")\n",
    "    elif sentiment_score['compound'] <= -0.05:\n",
    "        print(\"Negative\")\n",
    "    else:\n",
    "        print(\"Neutral\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Video ID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>Here in NZ 50% of retailers donâ€™t even have co...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>I will forever acknowledge this channel with t...</td>\n",
       "      <td>161.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>Whenever I go to a place that doesnâ€™t take App...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>Apple Pay is so convenient, secure, and easy t...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>wAZZ-UWGVHI</td>\n",
       "      <td>We only got Apple Pay in South Africa in 2020/...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     Video ID                                            Comment  Likes  \\\n",
       "0   1  wAZZ-UWGVHI  Here in NZ 50% of retailers donâ€™t even have co...   19.0   \n",
       "1   2  wAZZ-UWGVHI  I will forever acknowledge this channel with t...  161.0   \n",
       "2   3  wAZZ-UWGVHI  Whenever I go to a place that doesnâ€™t take App...    8.0   \n",
       "3   4  wAZZ-UWGVHI  Apple Pay is so convenient, secure, and easy t...   34.0   \n",
       "4   6  wAZZ-UWGVHI  We only got Apple Pay in South Africa in 2020/...   29.0   \n",
       "\n",
       "   Sentiment  \n",
       "0        0.0  \n",
       "1        2.0  \n",
       "2        0.0  \n",
       "3        2.0  \n",
       "4        2.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = []\n",
    "# y = []\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# bayes_classifier = BernoulliNB()\n",
    "\n",
    "# bayes_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-string value: nan\n",
      "Accuracy: 51.76%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "df = pd.read_csv('Comments.csv')\n",
    "\n",
    "comments = df['Comment'].tolist()\n",
    "sentiments = df['Sentiment'].tolist()\n",
    "\n",
    "training = dict(zip(comments, sentiments))\n",
    "\n",
    "# Convert the dictionary to a list of tuples\n",
    "# Each tuple contains a comment and the corresponding sentiment label\n",
    "data_tuples = [(comment, sentiment) for comment, sentiment in training.items()]\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_size = int(len(data_tuples) * 0.8)\n",
    "train_set, test_set = data_tuples[:train_size], data_tuples[train_size:]\n",
    "\n",
    "# Define a simple feature extractor\n",
    "def word_features(comment):\n",
    "    # Check if the comment is a string\n",
    "    if isinstance(comment, str):\n",
    "        return dict((word, True) for word in comment.split())\n",
    "    else:\n",
    "        # Handle non-string values (e.g., floats)\n",
    "        print(f\"Skipping non-string value: {comment}\")\n",
    "        return {}\n",
    "\n",
    "# Convert the data to the required format\n",
    "train_features = [(word_features(comment), sentiment) for (comment, sentiment) in train_set]\n",
    "test_features = [(word_features(comment), sentiment) for (comment, sentiment) in test_set]\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(train_features)\n",
    "\n",
    "# Evaluate accuracy\n",
    "acc = accuracy(classifier, test_features)\n",
    "print(f'Accuracy: {acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8302106027596223"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: implement\n",
    "n = df.shape[0]\n",
    "\n",
    "# P(Y=1) Probability of positive sentiment\n",
    "prob_pos_sent = (df['Sentiment'].sum()/2)/n\n",
    "# P(Y=0) Probability of negative sentiment\n",
    "prob_neg_sent = 1 - prob_pos_sent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 2794/18408 [00:03<00:18, 822.21it/s] "
     ]
    }
   ],
   "source": [
    "# vector with number of occurances of each word\n",
    "freq_vector = vectorize_comments(comments, bagOfWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorize_comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m comm\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# vector with number of occurances of each word\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m freq_vector \u001b[38;5;241m=\u001b[39m \u001b[43mvectorize_comments\u001b[49m(comments, bagOfWords)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# @ is dot product?\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# np.dot(freq_vector,comm)@(prob_pos_sent*freq_vector)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorize_comments' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO: run inference on comm\n",
    "_comm = \"I love you guys. I hope you know that. ðŸ˜‚ You guys brighten my day with these massive tech unboxing videos.\"\n",
    "comm = vectorize_comment(_comm, bagOfWords)\n",
    "comm\n",
    "\n",
    "# @ is dot product?\n",
    "# (freq_vector@comm)@(prob_pos_sent*freq_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
